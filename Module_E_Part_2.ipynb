{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTDcMptpTP70qiwUrsnMDD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kalvelagem/MAT421/blob/main/Module_E_Part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Section 3.4\n",
        "Logistic regression is a model that in its basic form uses a logistic function\n",
        "to model a binary dependent variable.\n",
        "\n",
        "Given the input data is of the form {(Œ±i,bi): i =1,...,n} where Œ±i ‚àà Rd are\n",
        "the features and bi ‚àà {0,1} is the label. \n",
        "\n",
        "a matrix representation: A ‚àà Rn√ód has rows Œ±Tj , j =1,...,n and b =$(b1,...,bn)^T$ ‚àà ${0,1}^n$. \n",
        "\n",
        "p(Œ±;X) = œÉ($Œ±^T$X) where œÉ(t) = $\\frac{1}{1+e^{-t}}$  for t ‚àà all real numbers\n",
        "\n",
        "Labels are giben by L(x;A,b)= $ùö∑_{i=1}^{n}$ $p(Œ±_i;X)^b$ $(1-p($a_i$;x))^{1-b}$\n",
        "\n",
        "Hessian: $‚àá_x^2$l(x;A,b) = $\\frac{1}{n}$ $‚àë_{i=1}^n$œÉ $(Œ±_i^Tx)(1-œÉ(Œ±_i^Tx))Œ±_iŒ±_i^T$ where $‚àá_x^2$ indicates the Hessian with respect tot he x variables. $Œ±_iŒ±_i^T$ is a symmetric matrix and PSD.\n",
        "\n",
        "To undate iteration formula: for step size Œí, one step of gradient descent is therefore $x^{k+1} = x^k + Œí \\frac{1}{n}‚àë_{i=1}^n (b_i-œÉ(a_i^Tx^k))Œ±_i$\n",
        "\n",
        "#Section 3.5\n",
        "k-means clustering is a popular method of vector quantization that aims to\n",
        "partition n observations into k clusters in which each observation belongs\n",
        "to the cluster with the nearest mean (cluster centers or cluster centroid),\n",
        "serving as a prototype of the cluster. k\n",
        "\n",
        "*   minimizes within-\n",
        "cluster variances (squared Euclidean distances), but not regular Euclidean\n",
        "distances. While k-means general converge quickly to a local optimum, the\n",
        "problem is computationally diÔ¨Äicult (NP-hard). \n",
        "*   aims to partition the n observations into k(<n)sets S =\n",
        "${S_1,...S_k }$ so as to minimize the within-cluster sum of squares (WCSS)\n",
        "\n",
        "The squared distance of each vector from its centroid summed over all the vectors: $WCSS_i =ùö∫_{x‚ààS_i}||x-Œº(S_i)||^2$\n",
        "\n",
        "where $Œº(S_i) is the mean of points in S_i$\n",
        "Œº(S) = $\\frac{1}{|S|}ùö∫_{x‚ààS}X$\n",
        "\n",
        "The objective is to find: arg $min_sùö∫_{i=1}^kWCSS_i$\n",
        "\n",
        "K-means Clustering Algorithm:\n",
        "1. Clusters the data into k groups where k is predefined.\n",
        "2. Select k points at random as cluster centers\n",
        "3. Assign objects to their closest cluster center according to the Euclidean distance function\n",
        "4. Calculate the centroid or mean of all objects in each cluster\n",
        "5. Repeat steps 2,3 and 4 until the same points are assigned to each cluster in consecutive rounds\n",
        "\n",
        "Final Equation: v = $\\frac{1}{|S_i|}ùö∫_{x=(x_j)‚ààS_i}x_j$ which is the componentwise definition of the centroid. \n",
        "* we minimize $WCSS_i$ when the old centroid is replaced with the new centroid. The sum of the WCSS_i must then also decrease during recomputation\n",
        "\n",
        "\n",
        "#Section 3.6 \n",
        "Support-vector machines (SVMs) are supervised learning models in machine\n",
        "learning, which aim to analyze data for classification and regression analysis. \n",
        "\n",
        "Objective: to find a hyperplane in a high dimensional space of the number of features that distinctly classifies the data points.\n",
        "\n",
        "$(x_1,y_1),...,(x_n,y_n)$\n",
        "where n is the number of points and $y_i$ is either 1 or -1, each indicating the class to which the point $x_i$ belongs. Each $x_i$ is a p-dimensiona real vector.\n",
        "\n",
        "Want to maximize the margin distance of hyperplanes that divides the group of points $x_i$ for which $y_1=1$ from the group of points for which $y_i = -1$. Maximizing the margin distance provides some reinforcement so that future data points can be classified with more confidence.\n",
        "\n",
        "A hyper plane can be written as the set of points x satisying $w^Tx-b = 0$ where w is the normal vector to the hyperplane. \n",
        "\n",
        "If the training data is linearly\n",
        "separable, we can select two parallel hyperplanes that separate the two classes\n",
        "of data, so that the distance between them is as large as possible. The region\n",
        "bounded by these two hyperplanes is called the ‚Äùmargin‚Äù, and the maximum-\n",
        "margin hyperplane is the hyperplane that lies halfway between them\n",
        "\n",
        "We are interested in two regions: anything on or above this\n",
        "boundary is of one class, with label 1 and anything on or below this boundary\n",
        "is of the other class, with label -1. The two hyperplanes can be respectively\n",
        "described by the equations\n",
        "\n",
        "$w^Tx-b = 1$\n",
        "\n",
        "$w^Tx-b = -1$\n",
        "\n",
        "Margin which can be expressed as for each i either $ w^Tx_i - b>=1 if y_i = 1$ or $w^Tx_i-b<=-1 if y_i = -1. If you combine them together youget $y_i(w^Tx_i-b)>= for all 1<=i<=n.\n",
        "\n",
        "The goal of the optimization then is to minimize: $min_{w,b}<Œª||w||^2 + \\frac{1}{n}‚àë_{i=1}^n max(0,1 - y_i(<w,x_i>-b))>$\n",
        "\n",
        "which minimizes ||w|| subject to $y_i(w^Tx_i-b)>=1 for all 1<=i<=n. Regularizatin term which arises directly from the margin: Œª||w||^2. Œª adjusts for the trade-off between increasing the margin size and ensuring that $x_i$ lie on the correct side of the margin while we choose the distance of the two hyperplanes to be $\\frac{2}{||w||}$\n",
        "\n",
        "For unconstrained optimization problem can be directly solved with gradient descent methods. This function is convex in the w we can easily apply a gradient descent method to find the minimum. For example, wiht stochastic gradient descent pick an i at random and update according to \n",
        "\n",
        "New b = old b - Œ≤($y_i$ if 1 - $y_i(w^Tx_i-b)$>0 otherwise it is 0) \n",
        "\n",
        "and \n",
        "\n",
        "new w = old w - Œ≤ (2Œªw - $\\frac{1}{n}y_ix_i$ if 1-$y_i(w^Tx_i-b)>0$ otherwise it is 2Œªw)"
      ],
      "metadata": {
        "id": "5AMbb_QmdkTO"
      }
    }
  ]
}